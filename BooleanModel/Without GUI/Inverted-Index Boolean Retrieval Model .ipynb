{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import all Documents\n",
    "D={}\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    file=\"Documents/\"+str(x)+\".txt\"\n",
    "    D[x]=open(file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appended all documents to allDoc\n",
    "allDoc=\"\"\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    allDoc=allDoc+\" \\n\"+D[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------Tokenization------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144647\n"
     ]
    }
   ],
   "source": [
    "#Tokens\n",
    "tokens=nltk.word_tokenize(allDoc)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12199\n"
     ]
    }
   ],
   "source": [
    "#Unique Tokens \n",
    "tokens=list(set(tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "tokens=[x.translate(removetable) for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Stoplist\n",
    "stopWord=open(\"Stopword-List.txt\").read()\n",
    "stopWord=nltk.word_tokenize(stopWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12006\n"
     ]
    }
   ],
   "source": [
    "#Removing Stop Words\n",
    "tokens=set(tokens)-set(stopWord)\n",
    "tokens=list(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12006\n"
     ]
    }
   ],
   "source": [
    "#Decapitalized\n",
    "tokens=[element.lower() for element in tokens]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorted Tokens\n",
    "tokens=sorted(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------END------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------DOCument Wise Tokenization----------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document wise Tokenization\n",
    "docToken={}\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=nltk.word_tokenize(D[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique Token Doc wise\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=set(docToken[x])\n",
    "    docToken[x]=list(docToken[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters Doc Wise\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=[y.translate(removetable) for y in docToken[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc wise sorted\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=sorted(docToken[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decaptilized Doc Wise\n",
    "for x in range(50):\n",
    "    x=x+1\n",
    "    docToken[x]=[element.lower() for element in docToken[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------END------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------posting List-----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "postinglist={}\n",
    "lst=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for x in range(len(tokens)):\n",
    "    lst=[]\n",
    "    for y in range(50):\n",
    "        y=y+1\n",
    "        \n",
    "        if tokens[x] in docToken[y]:\n",
    "            lst.append(y)\n",
    "            \n",
    "    postinglist[x]=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------Making Dictnory---------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "for x in range(len(tokens)):\n",
    "    dict.update({tokens[x]:postinglist[x]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict.csv', 'w',newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in dict.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------Query---------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"heard and noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heard', 'and', 'noise']\n"
     ]
    }
   ],
   "source": [
    "qt=nltk.word_tokenize(query)\n",
    "print(qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Special characters\n",
    "removetable=str.maketrans(\"\", \"\", \"'!@#$%^&*()_=-\\|][:';:,<.>/?`~\")\n",
    "qt=[x.translate(removetable) for x in qt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#Decapitalized\n",
    "qt=[element.lower() for element in qt]\n",
    "print(len(qt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for the Query : \n",
      "heard and noise\n",
      "Total documents retrieved : 12\n",
      "Documents: \n",
      "{4, 37, 8, 9, 11, 13, 46, 49, 25, 27, 28, 30}\n"
     ]
    }
   ],
   "source": [
    "ans=[]\n",
    "temp=[]\n",
    "if len(qt) == 1:\n",
    "    try:\n",
    "        ans=dict[qt[0]]\n",
    "    except KeyError:\n",
    "        print(\"Key Error\")\n",
    "    if not ans:\n",
    "        printString = \"Result for the Query : \" + qt[0]\n",
    "        print (\"0 documents returned as there is no match\")\n",
    "\n",
    "    else:\n",
    "        print (\"Result for the Query : \" + qt[0])\n",
    "        print (\"Total documents retrieved : \" + str(len(ans)))\n",
    "        print(\"Documents: \")\n",
    "        print(ans)\n",
    "                \n",
    "                \n",
    "else:\n",
    "    x=0\n",
    "    for i in range(len(qt)):\n",
    "        \n",
    "        if (x>(len(qt))+i-3):\n",
    "            break   \n",
    "        if (qt[i+1]==\"and\"):\n",
    "            try:\n",
    "                temp=set(dict[qt[i]]).intersection(set(dict[qt[i+2]]))\n",
    "                ans.append(temp)\n",
    "            except KeyError:\n",
    "                print(\"Key Error\")\n",
    "                ans.append([])\n",
    "            #------------------------------#\n",
    "                \n",
    "        elif (qt[i+1]==\"or\"):\n",
    "            try:\n",
    "                temp=set(dict[qt[i]]).union(set(dict[qt[i+2]]))\n",
    "                ans.append(temp)\n",
    "            except KeyError:\n",
    "                print(\"Key Error\")\n",
    "                ans.append([])\n",
    "            \n",
    "                #---------------------------------#\n",
    "        elif (qt[i+1]==\"not\"):\n",
    "            try:\n",
    "                ans.append(set(dict[qt[i]]).difference(set(dict[qt[i+2]])))\n",
    "            except KeyError:\n",
    "                print(\"Key Error\")\n",
    "                ans.append([])\n",
    "        x=x+2\n",
    "            \n",
    "        \n",
    "    print (\"Result for the Query : \")\n",
    "    print(query)\n",
    "    print (\"Total documents retrieved : \" + str(len(ans[-1])))\n",
    "    print(\"Documents: \")\n",
    "    print(ans[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
